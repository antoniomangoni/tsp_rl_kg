{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def calculate_reward(self):\n",
    "        logger.info(\"Calculating reward...\")\n",
    "        agent_pos = (self.agent_controler.agent.grid_x, self.agent_controler.agent.grid_y)\n",
    "        reward = 0\n",
    "\n",
    "        # Check if agent reached a new outpost\n",
    "        if agent_pos in self.outpost_coords and agent_pos not in self.outposts_visited:\n",
    "            reward += self.new_outpost_reward\n",
    "            self.outposts_visited.add(agent_pos)\n",
    "            logger.info(f\"Agent reached new outpost. Reward: {self.new_outpost_reward}\")\n",
    "\n",
    "            self.recent_path.clear()  # Clear the path memory when reaching a new outpost\n",
    "            if len(self.outposts_visited) == len(self.outpost_coords):\n",
    "                reward += self.completion_reward\n",
    "                logger.info(f\"All outposts visited. Additional reward: {self.completion_reward}\")\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            unvisited_outposts = [outpost for outpost in self.outpost_coords if outpost not in self.outposts_visited]\n",
    "            if unvisited_outposts:\n",
    "                current_min_distance = min(manhattan_distance(agent_pos, outpost) for outpost in unvisited_outposts)\n",
    "                \n",
    "                if current_min_distance < self.previous_min_distance:\n",
    "                    reward += self.closer_to_outpost_reward\n",
    "                    logger.info(f\"Agent moved closer to an outpost. Reward: {self.closer_to_outpost_reward}\")\n",
    "                elif current_min_distance > self.previous_min_distance:\n",
    "                    reward += self.farther_from_outpost_penalty\n",
    "                    logger.info(f\"Agent moved away from outposts. Penalty: {self.farther_from_outpost_penalty}\")\n",
    "                \n",
    "                self.previous_min_distance = current_min_distance\n",
    "            else:\n",
    "                logger.warning(\"No unvisited outposts left. The agent should have stopped already.\")\n",
    "\n",
    "            # Check for circular behavior\n",
    "            if agent_pos in self.recent_path:\n",
    "                reward += self.circular_behavior_penalty\n",
    "                logger.info(f\"Agent repeated a path. Penalty: {self.circular_behavior_penalty}\")\n",
    "            \n",
    "            # Update recent path memory\n",
    "            self.recent_path.append(agent_pos)\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, game_manager_args, simulation_manager_args, model_args):\n",
    "        self.game_manager = GameManager(**game_manager_args)\n",
    "        # ... rest of the initialization ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class GameManager:\n",
    "    def __init__(self, num_tiles, screen_size, vision_range, kg_completeness):\n",
    "        self.num_tiles = num_tiles\n",
    "        self.screen_size = screen_size\n",
    "        self.vision_range = vision_range\n",
    "        self.kg_completeness = kg_completeness\n",
    "        # ... rest of the initialization ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "import logging\n",
    "import traceback\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "from custom_env import CustomEnv\n",
    "from agent_model import AgentModel\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, log_file='training.log'):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # File handler\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        \n",
    "        # Formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "        \n",
    "        # Add handlers to logger\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "        \n",
    "        warnings.filterwarnings(\"always\")\n",
    "\n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def warning(self, message):\n",
    "        self.logger.warning(message)\n",
    "\n",
    "    def error(self, message):\n",
    "        self.logger.error(message)\n",
    "\n",
    "class EnvironmentManager:\n",
    "    def __init__(self, game_manager_args, simulation_manager_args, model_args):\n",
    "        self.game_manager_args = game_manager_args\n",
    "        self.simulation_manager_args = simulation_manager_args\n",
    "        self.model_args = model_args\n",
    "\n",
    "    def make_env(self):\n",
    "        env = CustomEnv(self.game_manager_args, self.simulation_manager_args, self.model_args)\n",
    "        return Monitor(env)\n",
    "\n",
    "    def set_kg_completeness(self, env, completeness):\n",
    "        env.set_kg_completeness(completeness)\n",
    "\n",
    "# ... [Rest of the classes remain the same] ...\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['PYGAME_DETECT_AVX2'] = '1'\n",
    "\n",
    "    base_config = {\n",
    "        'model_args': {'num_actions': 11},\n",
    "        'simulation_manager_args': {'number_of_environments': 10, 'number_of_curricula': 3},\n",
    "        'game_manager_args': {'num_tiles': 8, 'screen_size': 200, 'vision_range': 1, 'kg_completeness': 1.0},  # Add kg_completeness here\n",
    "        'model_config': {\n",
    "            'n_steps': 2048,\n",
    "            'batch_size': 64,\n",
    "            'learning_rate': 3e-4,\n",
    "            'gamma': 0.99\n",
    "        },\n",
    "        'total_timesteps': 1000000\n",
    "    }\n",
    "\n",
    "    kg_completeness_values = [0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "    logger = Logger('ablation_study.log')  # Specify the log file name\n",
    "    ablation_study = AblationStudy(base_config, kg_completeness_values, logger)\n",
    "\n",
    "    try:\n",
    "        ablation_study.run()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during the ablation study: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from custom_env import CustomEnv\n",
    "from agent_model import AgentModel\n",
    "import logging\n",
    "import traceback\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# ... (previous imports and setup remain the same)\n",
    "\n",
    "class TrainingManager:\n",
    "    # ... (previous methods remain the same)\n",
    "\n",
    "    def run_ablation_study(self, kg_completeness_levels):\n",
    "        results = {}\n",
    "        base_env = self.create_env(1.0)  # Create a base environment with full knowledge graph\n",
    "\n",
    "        for kg_completeness in kg_completeness_levels:\n",
    "            logger.info(f\"Starting ablation study for KG completeness: {kg_completeness}\")\n",
    "            \n",
    "            # Create a deep copy of the base environment\n",
    "            env = copy.deepcopy(base_env)\n",
    "            env.unwrapped.kg_completeness = kg_completeness  # Update the kg_completeness of the copied environment\n",
    "            \n",
    "            model = self.create_model(env)\n",
    "            \n",
    "            episode_rewards, episode_lengths = self.train_model(model, env)\n",
    "            \n",
    "            model.save(f\"ppo_custom_env_kg_{kg_completeness}\")\n",
    "            mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "            \n",
    "            results[kg_completeness] = {\n",
    "                'episode_rewards': episode_rewards,\n",
    "                'episode_lengths': episode_lengths,\n",
    "                'final_mean_reward': mean_reward,\n",
    "                'final_std_reward': std_reward\n",
    "            }\n",
    "            \n",
    "            env.close()\n",
    "            \n",
    "            logger.info(f\"Completed ablation study for KG completeness: {kg_completeness}\")\n",
    "            logger.info(f\"Final evaluation: Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        \n",
    "        base_env.close()  # Close the base environment\n",
    "        return results\n",
    "\n",
    "    # ... (other methods remain the same)\n",
    "\n",
    "def main():\n",
    "    # ... (setup remains the same)\n",
    "\n",
    "    training_manager = TrainingManager(model_args, simulation_manager_args, game_manager_args)\n",
    "    kg_completeness_levels = [0.2, 0.5, 0.8, 1.0]  # Example levels for ablation study\n",
    "\n",
    "    try:\n",
    "        results = training_manager.run_ablation_study(kg_completeness_levels)\n",
    "        training_manager.save_results(results)\n",
    "        logger.info(\"Ablation study completed and results saved.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in the main function: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pygame\n",
    "\n",
    "# Modified SimulationManager class\n",
    "class SimulationManager:\n",
    "    def __init__(self, game_manager_args, number_of_environments=1, number_of_curricula=1, plot=False):\n",
    "        self.base_game_managers = []\n",
    "        self.curriculum_indices = []\n",
    "        self.create_base_games(number_of_environments, game_manager_args, plot)\n",
    "        self.curriculum_indices, self.step_size = self.get_curriculum(number_of_curricula)\n",
    "        self.kg_completeness_environments = {}\n",
    "\n",
    "    def create_base_games(self, number_of_games, game_manager_args, plot):\n",
    "        num_tiles = game_manager_args['num_tiles']\n",
    "        screen_size = game_manager_args['screen_size']\n",
    "        vision_range = game_manager_args['vision_range']\n",
    "        base_kg_completeness = 1.0  # Create base environments with full knowledge\n",
    "\n",
    "        for _ in range(number_of_games):\n",
    "            game_manager = GameManager(num_tiles, screen_size, base_kg_completeness, vision_range, plot)\n",
    "            if len(game_manager.environment.outpost_locations) >= 3:\n",
    "                self.base_game_managers.append(game_manager)\n",
    "\n",
    "    def create_kg_completeness_environments(self, kg_completeness_levels):\n",
    "        for kg_completeness in kg_completeness_levels:\n",
    "            self.kg_completeness_environments[kg_completeness] = []\n",
    "            for base_gm in self.base_game_managers:\n",
    "                new_gm = copy.deepcopy(base_gm)\n",
    "                new_gm.kg_completeness = kg_completeness\n",
    "                new_gm.init_knowledge_graph()  # Reinitialize KG with new completeness\n",
    "                self.kg_completeness_environments[kg_completeness].append(new_gm)\n",
    "\n",
    "    # ... (other methods remain the same)\n",
    "\n",
    "# Modified CustomEnv class\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, simulation_manager, kg_completeness, model_args):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.simulation_manager = simulation_manager\n",
    "        self.kg_completeness = kg_completeness\n",
    "        self.game_managers = simulation_manager.kg_completeness_environments[kg_completeness]\n",
    "        self.current_game_index = -1\n",
    "        self.set_current_game_manager()\n",
    "\n",
    "        # ... (rest of the initialization remains the same)\n",
    "\n",
    "    def set_current_game_manager(self):\n",
    "        self.current_game_index = (self.current_game_index + 1) % len(self.game_managers)\n",
    "        self.current_gm = self.game_managers[self.current_game_index]\n",
    "        # Initialize pygame and set up the game\n",
    "        pygame.init()\n",
    "        self.current_gm.initialise_rendering()\n",
    "        self.environment = self.current_gm.environment\n",
    "        self.agent_controler = self.current_gm.agent_controler\n",
    "        self.kg = self.current_gm.kg_class\n",
    "        self.outpost_coords = self.environment.outpost_locations\n",
    "        self.best_route_energy = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # ... (existing reset logic)\n",
    "        self.set_current_game_manager()\n",
    "        # ... (rest of the reset method)\n",
    "\n",
    "    # ... (other methods remain the same)\n",
    "\n",
    "# Modified TrainingManager class\n",
    "class TrainingManager:\n",
    "    def __init__(self, model_args, simulation_manager_args, game_manager_args):\n",
    "        self.model_args = model_args\n",
    "        self.simulation_manager = SimulationManager(\n",
    "            game_manager_args,\n",
    "            simulation_manager_args['number_of_environments'],\n",
    "            simulation_manager_args['number_of_curricula']\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "    def create_env(self, kg_completeness):\n",
    "        env = CustomEnv(self.simulation_manager, kg_completeness, self.model_args)\n",
    "        return Monitor(env)\n",
    "\n",
    "    def run_ablation_study(self, kg_completeness_levels):\n",
    "        # Create environments for all KG completeness levels at the start\n",
    "        self.simulation_manager.create_kg_completeness_environments(kg_completeness_levels)\n",
    "\n",
    "        results = {}\n",
    "        for kg_completeness in kg_completeness_levels:\n",
    "            logger.info(f\"Starting ablation study for KG completeness: {kg_completeness}\")\n",
    "            env = self.create_env(kg_completeness)\n",
    "            model = self.create_model(env)\n",
    "            \n",
    "            episode_rewards, episode_lengths = self.train_model(model, env)\n",
    "            \n",
    "            model.save(f\"ppo_custom_env_kg_{kg_completeness}\")\n",
    "            mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "            \n",
    "            results[kg_completeness] = {\n",
    "                'episode_rewards': episode_rewards,\n",
    "                'episode_lengths': episode_lengths,\n",
    "                'final_mean_reward': mean_reward,\n",
    "                'final_std_reward': std_reward\n",
    "            }\n",
    "            \n",
    "            env.close()\n",
    "            \n",
    "            logger.info(f\"Completed ablation study for KG completeness: {kg_completeness}\")\n",
    "            logger.info(f\"Final evaluation: Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    model_args = {'num_actions': 11}\n",
    "    simulation_manager_args = {'number_of_environments': 100, 'number_of_curricula': 20}\n",
    "    game_manager_args = {'num_tiles': 16, 'screen_size': 200, 'vision_range': 2}\n",
    "\n",
    "    training_manager = TrainingManager(model_args, simulation_manager_args, game_manager_args)\n",
    "    kg_completeness_levels = [0.2, 0.5, 0.8, 1.0]  # Example levels for ablation study\n",
    "\n",
    "    try:\n",
    "        results = training_manager.run_ablation_study(kg_completeness_levels)\n",
    "        training_manager.save_results(results)\n",
    "        logger.info(\"Ablation study completed and results saved.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in the main function: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified SimulationManager class\n",
    "class SimulationManager:\n",
    "    def __init__(self, game_manager_args, number_of_environments=1, number_of_curricula=1, plot=False):\n",
    "        self.game_managers = []\n",
    "        self.curriculum_indices = []\n",
    "        self.create_games(number_of_environments, game_manager_args, plot)\n",
    "        self.curriculum_indices, self.step_size = self.get_curriculum(number_of_curricula)\n",
    "        self.kg_completeness = game_manager_args['kg_completeness']\n",
    "\n",
    "    # ... (other methods remain the same)\n",
    "\n",
    "    def set_kg_completeness(self, kg_completeness):\n",
    "        self.kg_completeness = kg_completeness\n",
    "\n",
    "# Modified CustomEnv class\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, game_manager_args, simulation_manager_args, model_args, plot=False):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        # ... (other initializations remain the same)\n",
    "\n",
    "        self.simulation_manager = SimulationManager(\n",
    "            game_manager_args,\n",
    "            simulation_manager_args['number_of_environments'], \n",
    "            simulation_manager_args['number_of_curricula'],\n",
    "            plot=plot\n",
    "        )\n",
    "        \n",
    "        self.current_game_index = -1 # set to -1 so reset increments to 0\n",
    "        self.set_current_game_manager()\n",
    "\n",
    "        # ... (rest of the initialization remains the same)\n",
    "\n",
    "    def set_current_game_manager(self):\n",
    "        if self.current_game_index >= len(self.simulation_manager.game_managers):\n",
    "            self.current_game_index = 0\n",
    "        self.current_gm = self.simulation_manager.game_managers[self.current_game_index]\n",
    "        # Initialize pygame and set up the game with the current kg_completeness\n",
    "        self.current_gm.start_ablation_game(self.simulation_manager.kg_completeness)\n",
    "        self.environment = self.current_gm.environment\n",
    "        self.agent_controler = self.current_gm.agent_controler\n",
    "        self.kg = self.current_gm.kg_class\n",
    "        self.outpost_coords = self.environment.outpost_locations\n",
    "        self.best_route_energy = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # ... (existing reset logic)\n",
    "        self.set_current_game_manager()  # This will call start_ablation_game\n",
    "        # ... (rest of the reset method)\n",
    "\n",
    "    # ... (other methods remain the same)\n",
    "\n",
    "# Modified TrainingManager class\n",
    "class TrainingManager:\n",
    "    def __init__(self, model_args, simulation_manager_args, game_manager_args):\n",
    "        self.model_args = model_args\n",
    "        self.simulation_manager_args = simulation_manager_args\n",
    "        self.game_manager_args = game_manager_args\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Create a single SimulationManager to be used across all ablation steps\n",
    "        self.simulation_manager = SimulationManager(\n",
    "            game_manager_args,\n",
    "            simulation_manager_args['number_of_environments'],\n",
    "            simulation_manager_args['number_of_curricula']\n",
    "        )\n",
    "\n",
    "    def create_env(self, kg_completeness):\n",
    "        # Set the new KG completeness in the SimulationManager\n",
    "        self.simulation_manager.set_kg_completeness(kg_completeness)\n",
    "        \n",
    "        env = CustomEnv(self.game_manager_args, self.simulation_manager_args, self.model_args)\n",
    "        env.simulation_manager = self.simulation_manager  # Use the existing SimulationManager\n",
    "        return Monitor(env)\n",
    "\n",
    "    # ... (other methods remain the same)\n",
    "\n",
    "    def run_ablation_study(self, kg_completeness_levels):\n",
    "        results = {}\n",
    "        for kg_completeness in kg_completeness_levels:\n",
    "            logger.info(f\"Starting ablation study for KG completeness: {kg_completeness}\")\n",
    "            env = self.create_env(kg_completeness)\n",
    "            model = self.create_model(env)\n",
    "            \n",
    "            episode_rewards, episode_lengths = self.train_model(model, env)\n",
    "            \n",
    "            model.save(f\"ppo_custom_env_kg_{kg_completeness}\")\n",
    "            mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "            \n",
    "            results[kg_completeness] = {\n",
    "                'episode_rewards': episode_rewards,\n",
    "                'episode_lengths': episode_lengths,\n",
    "                'final_mean_reward': mean_reward,\n",
    "                'final_std_reward': std_reward\n",
    "            }\n",
    "            \n",
    "            env.close()\n",
    "            \n",
    "            logger.info(f\"Completed ablation study for KG completeness: {kg_completeness}\")\n",
    "            logger.info(f\"Final evaluation: Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Main function remains the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
